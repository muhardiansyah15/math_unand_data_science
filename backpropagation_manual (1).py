# -*- coding: utf-8 -*-
"""Backpropagation Manual.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FzukwuD7XoRMLg5eVmLy3vZvJIMj0gq3
"""

#Edited by: Mutia Yollanda & Muhardiansyah
#2nd Meeting
#Tuesday, October 25th 2022
#Introduction of Artificial Intelligence

#Import the numpy package
import numpy as np

#Define the activation function
def sigmoid(x):
    return 1.0 / (1.0 + np.exp(-x))

def sigmoid_der(x):
  return sigmoid(x)*(1-sigmoid(x))#np.exp(-x)/((1.0+np.exp(-x))^2)

#Initialization of the networks
epochs = 61810
input_size, hidden_size, output_size = 2, 3, 1
learning_rate = 0.2

# Truth table XOR
X = np.array([[1,1], [1, 0], [0, 1], [0,0]])
Y = np.array([[0], [1], [1], [0]])

# Fill hidden and output layers with random values.
#w_hidden = np.random.uniform(size=(input_size, hidden_size))
#w_output = np.random.uniform(size=(hidden_size, output_size))

#or If you want to set manually the weights for each units, then
w_hidden = np.array([[0.2, 0.3, -0.1],[ 0.3, 0.1, -0.1]])
w_output = np.array([[0.5],[-0.3],[-0.4]])

# Learning iteration
for epoch in range(epochs):
    # Forward propagation
    actual_hidden = sigmoid(np.dot(X, w_hidden))
    output = np.dot(actual_hidden, w_output)

    # Calculate error (expected output - calculated output)
    error = Y - output

    # Backward Propagation
    dZ = error * learning_rate
    w_output += actual_hidden.T.dot(dZ)

    dH = dZ.dot(w_output.T) * sigmoid_der(actual_hidden)
    w_hidden += X.T.dot(dH)

#Adjusting the weights which is associated to each units (input-hidden or hidden-output)
actual_hidden = sigmoid(np.dot([0, 0], w_hidden))
actual_output = np.dot(actual_hidden, w_output)
print('[0, 0]', actual_output)

actual_hidden = sigmoid(np.dot([0, 1], w_hidden))
actual_output = np.dot(actual_hidden, w_output)
print('[0, 1]', actual_output)

actual_hidden = sigmoid(np.dot([1, 0], w_hidden))
actual_output = np.dot(actual_hidden, w_output)
print('[1, 0]', actual_output)

actual_hidden = sigmoid(np.dot([1, 1], w_hidden))
actual_output = np.dot(actual_hidden, w_output)
print('[1, 1]', actual_output)
# -*- coding: utf-8 -*-
"""Original Backpropagation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AIXrRBaxZKrvcNr9AwJioncWTtKWMDNK

#Edited by: Mutia Yollanda & Muhardiansyah### 
#6th Meeting: Tuesday, December 6th 2022
"""

# import files
from google.colab import files
uploaded = files.upload()

# install packages
import keras
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

df=pd.read_excel("IHK2.xlsx")

df

# drop the columns as it is no longer required
Y = df['IHK']
X = df[['KURS','INFLASI']]

Y

X

# deFIne train and test data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state
= None, shuffle=False)

# set ANN architecture
from keras.models import Sequential
from keras.layers import Dense
look_back = 2
def model_ann(look_back):
  model=Sequential()
  model.add(Dense(units=32, input_dim=look_back, activation='relu'))
  model.add(Dense(8, activation='relu'))
  model.add(Dense(1))
  model.compile(loss='mean_squared_error', optimizer='adam',metrics = ['mse',
  'mae'])
  return model

model_ann(2)

# model summary
model_ann(2).summary()

y_test

y_train

# model running process
from keras.callbacks import EarlyStopping
model=model_ann(look_back)
history=model.fit(X_train, y_train, epochs=5000, batch_size=20, verbose=0,
validation_data=(X_test, y_test), callbacks=[EarlyStopping(monitor='val_loss', 
patience=10)],shuffle=False)

# RMSE, MSE, and MAE value
train_score = model.evaluate(X_train, y_train, verbose=0)
print('Train Root Mean Squared Error(RMSE): %.3f; \n Train Mean Squared Error(MSE): %.3f; \n Train Mean Absolute Error(MAE) : %.3f ' % (np.sqrt(train_score[1]),
train_score[1], train_score[2]))
test_score = model.evaluate(X_test, y_test, verbose=0)
print('Test Root Mean Squared Error(RMSE): %.3f; \n Test Mean Squared Error(MSE): %.3f; \n Test Mean Absolute Error(MAE) : %.3f ' % (np.sqrt(test_score[1]),
test_score[1], test_score[2]))

# deFIne the loss plot
def model_loss(history):
  plt.figure(figsize=(8,4))
  plt.plot(history.history['loss'], label='Train_Loss')
  plt.plot(history.history['val_loss'], label='Test_Loss')
  plt.title('model_loss')
  plt.ylabel('loss')
  plt.xlabel('epochs')
  plt.legend(loc='upper right')
  plt.show()

# model predictions
import seaborn as sns
def prediction_plot(y_test, test_predict):
  len_prediction=[x for x in range(len(y_test))]
  plt.figure(figsize=(8,4))
  plt.plot(len_prediction, y_test, marker='.', label="actual")
  plt.plot(len_prediction, test_predict, 'r', label="prediction")
  plt.tight_layout()
  sns.despine(top=True)
  plt.subplots_adjust(left=0.07)
  plt.ylabel('y value', size=15)
  plt.xlabel('data', size=15)
  plt.legend(fontsize=15)
  plt.show()

# train and test prediction and prediction plot
test_predict1=model.predict(X_train)
test_predict2=model.predict(X_test)
prediction_plot(y_train, test_predict1)
prediction_plot(y_test, test_predict2)
np.set_printoptions(precision=3, suppress=True)
test_predict1
test_predict2

# data prediction and prediction plot
y_pred = np.concatenate((test_predict1, test_predict2))
print(y_pred)
prediction_plot(Y, y_pred)

# weight
weights=model.layers[0].get_weights()[0]
weights

len(weights)

biases=model.layers[0].get_weights()[1]
biases

len(biases)

weights1=model.layers[1].get_weights()[0]
weights1

len(weights1)

biases1=model.layers[1].get_weights()[1]
biases1

weights2=model.layers[2].get_weights()[0]
weights2

biases2=model.layers[2].get_weights()[1]
biases2